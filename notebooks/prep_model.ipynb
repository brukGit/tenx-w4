{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Model Builder and Preprocessor Notebook\n",
    "\n",
    "This notebook combines the functionality from `model_builder.py` and `preprocessor.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easter(year):\n",
    "    a = year % 19\n",
    "    b = year // 100\n",
    "    c = year % 100\n",
    "    d = b // 4\n",
    "    e = b % 4\n",
    "    f = (b + 8) // 25\n",
    "    g = (b - f + 1) // 3\n",
    "    h = (19 * a + b - d - g + 15) % 30\n",
    "    i = c // 4\n",
    "    k = c % 4\n",
    "    l = (32 + 2 * e + 2 * i - h - k) % 7\n",
    "    m = (a + 11 * h + 22 * l) // 451\n",
    "    month = (h + l - 7 * m + 114) // 31\n",
    "    day = ((h + l - 7 * m + 114) % 31) + 1\n",
    "    return datetime(year, month, day)\n",
    "\n",
    "class PreprocessingError(Exception):\n",
    "    \"\"\"Custom exception for preprocessing errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.num_imputer = SimpleImputer(strategy='mean')\n",
    "        self.cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "        self.label_encoders = {}\n",
    "        self.onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        self.holidays = self._generate_holidays()\n",
    "        logging.info(f\"Initialized Preprocessor with {len(self.holidays)} holidays\")\n",
    "\n",
    "    def _generate_holidays(self):\n",
    "        holidays = []\n",
    "        for year in range(2013, 2016):\n",
    "            holidays.extend([\n",
    "                datetime(year, 1, 1),    # New Year's Day\n",
    "                easter(year),            # Easter\n",
    "                datetime(year, 10, 31),  # Halloween\n",
    "                datetime(year, 12, 25)   # Christmas\n",
    "            ])\n",
    "        return pd.to_datetime(holidays)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        logging.info(\"Starting preprocessing...\")\n",
    "        logging.info(f\"Initial DataFrame shape: {df.shape}\\n columns: {df.columns.tolist()}\")\n",
    "\n",
    "        try:\n",
    "            date_column = df['Date']  # Store the Date column separately\n",
    "            df = self._handle_missing_values_and_encode(df)\n",
    "            df['Date'] = date_column  # Add the Date column back\n",
    "            df = self._extract_datetime_features(df)\n",
    "        except PreprocessingError as e:\n",
    "            logging.error(f\"Preprocessing failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        logging.info(f\"Final DataFrame shape: {df.shape}\")\n",
    "        logging.info(\"Preprocessing completed.\")\n",
    "        logging.info(\"Data types after preprocessing:\")\n",
    "        return df\n",
    "\n",
    "    def _handle_missing_values_and_encode(self, df):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        logging.info(\"Handling missing values, encoding categorical variables, and scaling numerical variables...\")\n",
    "        \n",
    "        try:\n",
    "            # Define expected column types\n",
    "            expected_numerical_columns = ['Sales', 'Customers', 'Open', 'Promo', 'SchoolHoliday', 'CompetitionDistance', \n",
    "                                          'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
    "            expected_label_encode_columns = ['Store', 'DayOfWeek']\n",
    "            expected_onehot_encode_columns = ['StateHoliday', 'StoreType', 'Assortment', 'PromoInterval']\n",
    "            \n",
    "            # Filter columns that actually exist in the DataFrame\n",
    "            numerical_columns = [col for col in expected_numerical_columns if col in df.columns]\n",
    "            label_encode_columns = [col for col in expected_label_encode_columns if col in df.columns]\n",
    "            onehot_encode_columns = [col for col in expected_onehot_encode_columns if col in df.columns]\n",
    "            \n",
    "            # Handle numerical columns\n",
    "            for col in numerical_columns:\n",
    "                df[col] = self.num_imputer.fit_transform(df[[col]]).ravel()\n",
    "                df[col] = self.scaler.fit_transform(df[[col]]).ravel()\n",
    "\n",
    "            # Handle label encoding\n",
    "            for col in label_encode_columns:\n",
    "                df[col] = df[col].astype(str)  # Convert to string\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "                self.label_encoders[col] = le\n",
    "\n",
    "            # Handle one-hot encoding\n",
    "            df_onehot = pd.DataFrame()\n",
    "            for col in onehot_encode_columns:\n",
    "                df[col] = df[col].astype(str)  # Convert to string\n",
    "                df[col] = self.cat_imputer.fit_transform(df[[col]]).ravel()\n",
    "                onehot_cols = self.onehot.fit_transform(df[[col]])\n",
    "                onehot_df = pd.DataFrame(onehot_cols, columns=[f\"{col}_{cat}\" for cat in self.onehot.categories_[0]])\n",
    "                df_onehot = pd.concat([df_onehot, onehot_df], axis=1)\n",
    "\n",
    "            # Combine all processed columns\n",
    "            df_processed = pd.concat([df[numerical_columns + label_encode_columns], df_onehot], axis=1)\n",
    "            \n",
    "            logging.info(f\"Missing value handling, encoding, and scaling completed in {time.time() - start_time:.2f} seconds\")\n",
    "            logging.info(f\"DataFrame shape after processing: {df_processed.shape}\")\n",
    "            \n",
    "            return df_processed\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in _handle_missing_values_and_encode: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            raise PreprocessingError(error_msg)\n",
    "    \n",
    "    def _extract_datetime_features(self, df):\n",
    "        try:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Year'] = df['Date'].dt.year\n",
    "            df['Month'] = df['Date'].dt.month\n",
    "            df['Day'] = df['Date'].dt.day\n",
    "            df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in _extract_datetime_features: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            raise PreprocessingError(error_msg)\n",
    "\n",
    "    def _encode_categorical_features(self, df):\n",
    "        logging.info(\"Encoding categorical features...\")\n",
    "        categorical_columns = ['StoreType', 'Assortment', 'StateHoliday', 'MonthPeriod']\n",
    "        return pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "    def _scale_numerical_features(self, df):        \n",
    "        logging.info(\"Scaling numerical features...\")\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numerical_columns] = self.scaler.fit_transform(df[numerical_columns])\n",
    "        return df\n",
    "\n",
    "    def inverse_transform_sales(self, sales):\n",
    "        return self.scaler.inverse_transform(sales.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelBuilder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilderError(Exception):\n",
    "    \"\"\"Custom exception for model building errors\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelBuilder:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, X, y):\n",
    "        logging.info(\"Building model...\")\n",
    "        \n",
    "        # Check if 'Date' column is present in X\n",
    "        if 'Date' in X.columns:\n",
    "            logging.warning(\"'Date' column found in feature set. This column will be dropped.\")\n",
    "            X = X.drop('Date', axis=1)\n",
    "        \n",
    "        # Check data types\n",
    "        non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "        if len(non_numeric_cols) > 0:\n",
    "            error_msg = f\"Non-numeric columns found in features: {non_numeric_cols.tolist()}\"\n",
    "            logging.error(error_msg)\n",
    "            raise ModelBuilderError(error_msg)\n",
    "        \n",
    "        # Log the final set of features being used\n",
    "        logging.info(f\"Features used for modeling: {X.columns.tolist()}\")\n",
    "        \n",
    "        param_dist = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "\n",
    "        rf = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "        self.model = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                        n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        try:\n",
    "            self.model.fit(X, y)\n",
    "            logging.info(f\"Best parameters: {self.model.best_params_}\")\n",
    "            return self.model\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in build_model: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            raise ModelBuilderError(error_msg)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        if self.model is not None:\n",
    "            try:\n",
    "                timestamp = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "                filename = f\"{path}/model_{timestamp}.pkl\"\n",
    "                joblib.dump(self.model, filename)\n",
    "                logging.info(f\"Model saved to {filename}\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error in save_model: {str(e)}\"\n",
    "                logging.error(error_msg)\n",
    "                raise